{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Quickstart for Azure AI Search\n",
    "\n",
    "This quickstart provides a query for RAG scenarios. It demonstrates an approach for grounding chat queries with data in a search index on Azure AI Search.\n",
    "\n",
    "We took a few shortcuts to keep the exercise basic and focused on query definitions:\n",
    "\n",
    "- We use the hotels-sample-index, which can be created in minutes and runs on any search service tier. This index is created by a wizard using built-in sample data.\n",
    "\n",
    "- We omit vectors so that we can skip chunking and embedding.\n",
    "\n",
    "Once you understand the fundamentals of integrating queries from Azure AI Search to an LLM, you can build on that experience by adding vector fields and vector and hybrid queries. We recommend the [phi-chat Python code example](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/phi-chat/phi-chat.ipynb) for that step.\n",
    "\n",
    "This quickstart is documented in [Quickstart: Generative search (RAG) with grounding data from Azure AI Search](https://learn.microsoft.com/azure/search/search-get-started-rag). If you need more guidance than the readme provides, please refer to the article.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource)\n",
    "\n",
    "  - Configure Azure OpenAI to use a system managed identity.\n",
    "  - Deploy a chat model (GPT-3.5-Turbo, GPT-4, or equivalent LLM).\n",
    "\n",
    "- [Azure AI Search](https://learn.microsoft.com/azure/search/search-create-service-portal)\n",
    "\n",
    "  - Basic tier or higher is recommended.\n",
    "  - Choose the same region as Azure OpenAI.\n",
    "  - Enable semantic ranking.\n",
    "  - Enable role-based access control.\n",
    "\n",
    "## Configure access\n",
    "\n",
    "This quickstart assumes authentication and authorization using Microsoft Entra ID and role assignments. It also assumes that you run this code from your local device.\n",
    "\n",
    "- On Azure AI Search, create a role assignment for the Azure OpenAI system managed identity. Required roles: **Search Index Data Reader**, **Search Service Contributor**.\n",
    "\n",
    "- Make sure you also have a role assignment that gives you permissions to create and query objects: Required roles: **Search Index Data Reader**, **Search Index Data Contributor**, **Search Service Contributor**.\n",
    "\n",
    "- On Azure OpenAI, create a role assigment for yourself to send requests from your local device: Required role: **Cognitive Services OpenAI User**.\n",
    "\n",
    "## Create the sample index\n",
    "\n",
    "This quickstart assumes the hotels-sample-index, which you can create in minutes using [this quickstart](https://learn.microsoft.com/azure/search/search-get-started-portal).\n",
    "\n",
    "Once the index exists, modify it in the Azure portal to use this semantic configuration:\n",
    "  \n",
    "    ```json\n",
    "    \"semantic\": {\n",
    "    \"defaultConfiguration\": \"semantic-config\",\n",
    "    \"configurations\": [\n",
    "      {\n",
    "        \"name\": \"semantic-config\",\n",
    "        \"prioritizedFields\": {\n",
    "          \"titleField\": { \"fieldName\": \"HotelName\" },\n",
    "          \"prioritizedContentFields\": [ { \"fieldName\": \"Description\" } ],\n",
    "          \"prioritizedKeywordsFields\": [\n",
    "            { \"fieldName\": \"Category\" },\n",
    "            { \"fieldName\": \"Tags\" }\n",
    "          ]}\n",
    "       }]},\n",
    "    ```\n",
    "\n",
    "Now that you have your Azure resources, an index, and model in place, you can run the script to chat with the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package install for quickstart\n",
    "! pip install azure-search-documents==11.6.0b4 --quiet\n",
    "! pip install azure-identity==1.16.0 --quiet\n",
    "! pip install openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set endpoints and deployment model\n",
    "AZURE_SEARCH_SERVICE: str = \"PUT YOUR SEARCH SERVICE ENDPOINT HERE\"\n",
    "AZURE_OPENAI_ACCOUNT: str = \"PUT YOUR AZURE OPENAI ENDPOINT HERE\"\n",
    "AZURE_DEPLOYMENT_MODEL: str = \"gpt-35-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set query parameters for grounding the conversation on your search index\n",
    "search_type=\"text\"\n",
    "use_semantic_reranker=True\n",
    "sources_to_include=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the clients, define a chat instance, create a search function\n",
    "from azure.core.credentials_async import AsyncTokenCredential\n",
    "from azure.identity.aio import get_bearer_token_provider\n",
    "from azure.search.documents.aio import SearchClient\n",
    "from openai import AsyncAzureOpenAI\n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "\n",
    "def create_openai_client(credential: AsyncTokenCredential) -> AsyncAzureOpenAI:\n",
    "    token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "    return AsyncAzureOpenAI(\n",
    "        api_version=\"2024-04-01-preview\",\n",
    "        azure_endpoint=AZURE_OPENAI_ACCOUNT,\n",
    "        azure_ad_token_provider=token_provider\n",
    "    )\n",
    "\n",
    "def create_search_client(credential: AsyncTokenCredential) -> SearchClient:\n",
    "    return SearchClient(\n",
    "        endpoint=AZURE_SEARCH_SERVICE,\n",
    "        index_name=\"hotels-sample-index\",\n",
    "        credential=credential\n",
    "    )\n",
    "\n",
    "# This quickstart is only using text at this time\n",
    "class SearchType(Enum):\n",
    "    TEXT = \"text\"\n",
    "    VECTOR = \"vector\"\n",
    "    HYBRID = \"hybrid\"\n",
    "\n",
    "# This function retrieves the sselected fields from the search index\n",
    "async def get_sources(search_client: SearchClient, query: str, search_type: SearchType, use_semantic_reranker: bool = True, sources_to_include: int = 5) -> List[str]:\n",
    "    search_type == SearchType.TEXT,\n",
    "    response = await search_client.search(\n",
    "        search_text=query,\n",
    "        query_type=\"semantic\" if use_semantic_reranker else \"simple\",\n",
    "        top=sources_to_include,\n",
    "        select=\"Description,HotelName,Tags\"\n",
    "    )\n",
    "\n",
    "    return [ document async for document in response ]\n",
    "\n",
    "GROUNDED_PROMPT=\"\"\"\n",
    "You are a friendly assistant that recommends hotels based on activities and amenities.\n",
    "Answer the query using only the sources provided below in a friendly and concise bulleted manner.\n",
    "Answer ONLY with the facts listed in the list of sources below.\n",
    "If there isn't enough information below, say you don't know.\n",
    "Do not generate answers that don't use the sources below.\n",
    "Query: {query}\n",
    "Sources:\\n{sources}\n",
    "\"\"\"\n",
    "class ChatThread:\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        self.search_results = []\n",
    "    \n",
    "    def append_message(self, role: str, message: str):\n",
    "        self.messages.append({\n",
    "            \"role\": role,\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    async def append_grounded_message(self, search_client: SearchClient, query: str, search_type: SearchType, use_semantic_reranker: bool = True, sources_to_include: int = 5):\n",
    "        sources = await get_sources(search_client, query, search_type, use_semantic_reranker, sources_to_include)\n",
    "        sources_formatted = \"\\n\".join([f'{document[\"HotelName\"]}:{document[\"Description\"]}:{document[\"Tags\"]}' for document in sources])\n",
    "        self.append_message(role=\"user\", message=GROUNDED_PROMPT.format(query=query, sources=sources_formatted))\n",
    "        self.search_results.append(\n",
    "            {\n",
    "                \"message_index\": len(self.messages) - 1,\n",
    "                \"query\": query,\n",
    "                \"sources\": sources\n",
    "            }\n",
    "        )\n",
    "\n",
    "    async def get_openai_response(self, openai_client: AsyncAzureOpenAI, model: str):\n",
    "        response = await openai_client.chat.completions.create(\n",
    "            messages=self.messages,\n",
    "            model=model\n",
    "        )\n",
    "        self.append_message(role=\"assistant\", message=response.choices[0].message.content)\n",
    "\n",
    "    def get_last_message(self) -> Optional[object]:\n",
    "        return self.messages[-1] if len(self.messages) > 0 else None\n",
    "\n",
    "    def get_last_message_sources(self) -> Optional[List[object]]:\n",
    "        return self.search_results[-1][\"sources\"] if len(self.search_results) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the chat thread and run the conversation\n",
    "import azure.identity.aio\n",
    "\n",
    "chat_thread = ChatThread()\n",
    "chat_deployment = AZURE_DEPLOYMENT_MODEL\n",
    "\n",
    "async with azure.identity.aio.DefaultAzureCredential() as credential, create_search_client(credential) as search_client, create_openai_client(credential) as openai_client:\n",
    "    await chat_thread.append_grounded_message(\n",
    "        search_client=search_client,\n",
    "        query=\"Can you recommend a few hotels near the ocean with beach access and good views\",\n",
    "        search_type=SearchType(search_type),\n",
    "        use_semantic_reranker=use_semantic_reranker,\n",
    "        sources_to_include=sources_to_include,\n",
    "        k=k)\n",
    "    await chat_thread.get_openai_response(openai_client=openai_client, model=chat_deployment)\n",
    "\n",
    "print(chat_thread.get_last_message()[\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
