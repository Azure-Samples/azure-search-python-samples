{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity.aio import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "# Variables not used here do not need to be updated in your .env file\n",
    "source_endpoint = os.environ[\"AZURE_SEARCH_SOURCE_SERVICE_ENDPOINT\"]\n",
    "# Using a key is optional. See https://learn.microsoft.com/en-us/azure/search/keyless-connections\n",
    "source_credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_SOURCE_ADMIN_KEY\")) if os.getenv(\"AZURE_SEARCH_SOURCE_ADMIN_KEY\") else DefaultAzureCredential()\n",
    "destination_endpoint = os.environ[\"AZURE_SEARCH_DESTINATION_SERVICE_ENDPOINT\"]\n",
    "destination_credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_DESTINATION_ADMIN_KEY\")) if os.getenv(\"AZURE_SEARCH_DESTINATION_ADMIN_KEY\") else DefaultAzureCredential()\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "timestamp_field_name = os.environ[\"AZURE_SEARCH_TIMESTAMP_FIELD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.aio import SearchIndexClient\n",
    "\n",
    "async def copy_index_definition(source_index_client: SearchIndexClient, destination_index_client: SearchIndexClient, index_name: str):\n",
    "    index = await source_index_client.get_index(index_name)\n",
    "    # Check for any synonym maps\n",
    "    synonym_map_names = []\n",
    "    for field in index.fields:\n",
    "        if field.synonym_map_names:\n",
    "            synonym_map_names.extend(field.synonym_map_names)\n",
    "    \n",
    "    # Copy over synonym maps if they exist\n",
    "    for synonym_map_name in synonym_map_names:\n",
    "        synonym_map = await source_index_client.get_synonym_map(synonym_map_name)\n",
    "        await destination_index_client.create_or_update_synonym_map(synonym_map)\n",
    "    \n",
    "    # Copy over the index\n",
    "    await destination_index_client.create_or_update_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_index_client = SearchIndexClient(endpoint=source_endpoint, credential=source_credential)\n",
    "destination_index_client = SearchIndexClient(endpoint=destination_endpoint, credential=destination_credential)\n",
    "\n",
    "await copy_index_definition(source_index_client, destination_index_client, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.aio import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SearchFieldDataType\n",
    "from typing import List\n",
    "\n",
    "async def validate_resume_backup_and_restore(index_client: SearchIndexClient, index_name: str, timestamp_field_name: str) -> bool:\n",
    "    index = await index_client.get_index(index_name)\n",
    "\n",
    "    found_field = False\n",
    "    for field in index.fields:\n",
    "        if field.name == timestamp_field_name:\n",
    "            found_field = True\n",
    "            if field.type != SearchFieldDataType.DateTimeOffset:\n",
    "                # Field must be a timestamp\n",
    "                return False\n",
    "            if not field.filterable:\n",
    "                # Field must be filterable\n",
    "                return False\n",
    "            if not field.sortable:\n",
    "                # Field must be sortable\n",
    "                return False\n",
    "            break\n",
    "    \n",
    "    # Field must exist on the index\n",
    "    return found_field\n",
    "\n",
    "async def validate_fields_backup_and_restore(index_client: SearchIndexClient, index_name: str) -> List[str]:\n",
    "    missing_fields = []\n",
    "    index = await index_client.get_index(index_name)\n",
    "    for field in index.fields:\n",
    "        message = \"\"\n",
    "        if not field.stored:\n",
    "            message += f\"Field {field.name} cannot be backed up because it's not marked as stored\\n\"\n",
    "        elif field.hidden: \n",
    "            message += f\"Field {field.name} cannot be backed up because it's not marked as retrievable\\n\"\n",
    "        \n",
    "        if message:\n",
    "            missing_fields.append(message)\n",
    "    \n",
    "    return missing_fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index has a valid timestamp field and can use resumable backup and restore\n"
     ]
    }
   ],
   "source": [
    "can_resume_backup_and_restore = await validate_resume_backup_and_restore(source_index_client, index_name, timestamp_field_name)\n",
    "if can_resume_backup_and_restore:\n",
    "    print(\"Index has a valid timestamp field and can use resumable backup and restore\")\n",
    "else:\n",
    "    print(\"Index does not have a valid timestamp field and cannot use resumable backup and restore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.aio import SearchClient\n",
    "from typing import Optional, AsyncGenerator, List, Callable, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "from uuid import uuid4\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "async def get_total_documents_remaining(client: SearchClient, timestamp_field_name: str, min_timestamp: Optional[str] = None, max_timestamp: Optional[str] = None) -> int:\n",
    "    filter = None\n",
    "    if min_timestamp and not max_timestamp:\n",
    "        filter = f\"{timestamp_field_name} ge {min_timestamp}\"\n",
    "    elif min_timestamp and max_timestamp:\n",
    "        filter = f\"{timestamp_field_name} ge {min_timestamp} and {timestamp_field_name} le {max_timestamp}\"\n",
    "    results = await client.search(\n",
    "        search_text=\"*\",\n",
    "        include_total_count=True,\n",
    "        filter=filter,\n",
    "        top=0\n",
    "    )\n",
    "    return await results.get_count()\n",
    "\n",
    "async def get_timestamp_bound(client: SearchClient, timestamp_field_name: str, max: bool) -> Optional[str]:\n",
    "    result = await client.search(\n",
    "        search_text=\"*\",\n",
    "        order_by=f\"{timestamp_field_name} {'desc' if max else 'asc'}\",\n",
    "        top=1,\n",
    "        select=[timestamp_field_name]\n",
    "    )\n",
    "    result = [item async for item in result]\n",
    "    if len(result) == 0:\n",
    "        return None\n",
    "    return result[0][timestamp_field_name]\n",
    "\n",
    "def timestamp_to_datetime(timestamp: str) -> datetime:\n",
    "    return datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "def datetime_to_timestamp(date: datetime) -> str:\n",
    "    # Trim microseconds to milliseconds\n",
    "    return date.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")[:-3] + \"Z\"\n",
    "    \n",
    "async def get_partition_bounds_backup_results(client: SearchClient, timestamp_field_name: str, desired_partitions: int = 2, partition_size_threshold: float = 0.05, min_timestamp: Optional[str] = None, max_timestamp: Optional[str] = None) -> List[datetime]:\n",
    "    if max_timestamp == None:\n",
    "        max_timestamp = await get_timestamp_bound(client, timestamp_field_name, max=True)\n",
    "        if max_timestamp == None:\n",
    "            return []\n",
    "    if min_timestamp == None:\n",
    "        min_timestamp = await get_timestamp_bound(client, timestamp_field_name, max=False)\n",
    "\n",
    "    if min_timestamp == max_timestamp or desired_partitions == 1:\n",
    "        return []\n",
    "\n",
    "    partition_splits = []\n",
    "    low = timestamp_to_datetime(min_timestamp)\n",
    "    for partition in range(desired_partitions - 1):\n",
    "        high = timestamp_to_datetime(max_timestamp)\n",
    "        remaining_partitions = desired_partitions - partition\n",
    "        target_partition_size = await get_total_documents_remaining(client, timestamp_field_name, min_timestamp=datetime_to_timestamp(low)) // remaining_partitions\n",
    "        partition_threshold = target_partition_size * partition_size_threshold\n",
    "        best_split = None\n",
    "        partition_sizes = []\n",
    "        mid = low + (high - low) / 2\n",
    "        while low <= high:\n",
    "            current_partition_size = await get_total_documents_remaining(client, timestamp_field_name, datetime_to_timestamp(low), datetime_to_timestamp(mid))\n",
    "            partition_sizes.append((mid, current_partition_size))\n",
    "            if current_partition_size < target_partition_size + partition_threshold and current_partition_size > target_partition_size - partition_threshold:\n",
    "                best_split = mid\n",
    "                break\n",
    "            elif current_partition_size < target_partition_size:\n",
    "                mid = mid + (high - mid) / 2\n",
    "            else:\n",
    "                high = mid\n",
    "                mid = mid - (mid - low) / 2\n",
    "        \n",
    "        if best_split is None:\n",
    "            print(\"Could not find best split....\", partition_sizes)\n",
    "            min_difference = -1\n",
    "            for split, partition_size in partition_sizes:\n",
    "                difference = abs(target_partition_size - partition_size)\n",
    "                if min_difference == -1 or difference < min_difference:\n",
    "                    best_split = split\n",
    "                    min_difference = difference\n",
    "                    print(\"set fallback \", partition_size)\n",
    "\n",
    "        partition_splits.append(best_split)\n",
    "        low = best_split + timedelta(milliseconds=1)\n",
    "\n",
    "    return partition_splits\n",
    "\n",
    "async def get_partitions(client: SearchClient, timestamp_field_name: str, partition_splits: List[datetime]) -> List[Tuple[str, str]]:\n",
    "    max_timestamp = await get_timestamp_bound(client, timestamp_field_name, max=True)\n",
    "    if max_timestamp == None:\n",
    "        return []\n",
    "    min_timestamp = await get_timestamp_bound(client, timestamp_field_name, max=False)\n",
    "    prev_partition_end = timestamp_to_datetime(min_timestamp)\n",
    "    partitions = []\n",
    "    for partition_split in partition_splits:\n",
    "        partitions.append((datetime_to_timestamp(prev_partition_end), datetime_to_timestamp(partition_split)))\n",
    "        prev_partition_end = partition_split + timedelta(milliseconds=1)\n",
    "    partitions.append((datetime_to_timestamp(prev_partition_end), max_timestamp))\n",
    "    return partitions\n",
    "\n",
    "async def resume_backup_results(client: SearchClient, timestamp_field_name: str, timestamp: Optional[str], select=None) -> AsyncGenerator[List[dict], None]:\n",
    "    session_id = str(uuid4())\n",
    "    max_results_size = 100000\n",
    "    get_next_results = True\n",
    "    while get_next_results:\n",
    "        total_results_size = 0\n",
    "        results = await client.search(\n",
    "            search_text=\"*\",\n",
    "            order_by=f\"{timestamp_field_name} asc\",\n",
    "            top=max_results_size,\n",
    "            filter=f\"{timestamp_field_name} ge {timestamp}\" if timestamp else None,\n",
    "            session_id=session_id,\n",
    "            select=select\n",
    "        )\n",
    "        results_by_page = results.by_page()\n",
    "\n",
    "        async for page in results_by_page:\n",
    "            next_page = [item async for item in page]\n",
    "            total_results_size += len(next_page)\n",
    "            yield next_page\n",
    "            timestamp = next_page[-1][timestamp_field_name]\n",
    "        \n",
    "        get_next_results = total_results_size == max_results_size\n",
    "\n",
    "async def get_key_field(index_client: SearchIndexClient, index_name: str) -> str:\n",
    "    index = await index_client.get_index(index_name)\n",
    "    for field in index.fields:\n",
    "        if field.key:\n",
    "            return field.name\n",
    "    \n",
    "    raise Exception(\"No key field in the index\")\n",
    "\n",
    "async def backup_index_with_resume(source_client: SearchClient, destination_client: SearchClient, timestamp_field_name: str, total_documents: int, resume_timestamp: Optional[str], backup_tasks:int = 2, on_backup_page: Optional[Callable[[str], None]] = None) -> None:\n",
    "    progress_bar = tqdm(total=total_documents, desc=\"Backing up documents...\", unit=\"docs\", unit_scale=False)\n",
    "    pages_label = widgets.Label(value=\"Queued Result Pages: 0\")\n",
    "    display(pages_label)\n",
    "    \n",
    "    async def get_results(output_queue: asyncio.Queue):\n",
    "        results = resume_backup_results(source_client, timestamp_field_name, timestamp=resume_timestamp)\n",
    "        async for result_page in results:\n",
    "            pages_label.value=f\"Queued Result Pages: {output_queue.qsize()}\"\n",
    "            await output_queue.put(result_page)\n",
    "        await output_queue.put(None)\n",
    "    \n",
    "    async def backup_results(results_queue: asyncio.Queue, timestamp_queue: asyncio.Queue):\n",
    "        processed_count = 0\n",
    "        while True:\n",
    "            result_page = await results_queue.get()\n",
    "            if result_page is None:\n",
    "                break\n",
    "            saved_timestamp = result_page[-1][timestamp_field_name]\n",
    "            await destination_client.upload_documents(result_page)\n",
    "            await timestamp_queue.put(saved_timestamp)\n",
    "            processed_count += len(result_page)\n",
    "            progress_bar.update(len(result_page))\n",
    "    \n",
    "    async def checkpoint_results(timestamp_queue: asyncio.Queue):\n",
    "        latest_timestamp = None\n",
    "        while True:\n",
    "            next_timestamp = await timestamp_queue.get()\n",
    "            if next_timestamp is None:  # Stop signal received\n",
    "                break\n",
    "            if latest_timestamp is None or next_timestamp > latest_timestamp:\n",
    "                latest_timestamp = next_timestamp\n",
    "                on_backup_page(latest_timestamp)\n",
    "\n",
    "    results_queue = asyncio.Queue()\n",
    "    timestamp_queue = asyncio.Queue()\n",
    "\n",
    "    # Run producer and consumer concurrently\n",
    "    producer_task = asyncio.create_task(get_results(results_queue))\n",
    "    \n",
    "    consumer_tasks = [asyncio.create_task(backup_results(results_queue, timestamp_queue)) for i in range(backup_tasks)]\n",
    "\n",
    "    checkpoint_task = asyncio.create_task(checkpoint_results(timestamp_queue))\n",
    "\n",
    "    # Wait for the producer to complete\n",
    "    await producer_task\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    await producer_task\n",
    "    await asyncio.gather(*consumer_tasks)\n",
    "    await checkpoint_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_client = SearchClient(source_endpoint, index_name, source_credential)\n",
    "destination_client = SearchClient(destination_endpoint, index_name, destination_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 2024-11-10T12:48:37.4980Z 2024-11-10T15:28:48.3827Z 1288000\n",
      "Partition 2024-11-10T15:28:48.3837Z 2024-11-10T19:29:04.7103Z 1194000\n",
      "Partition 2024-11-10T19:29:04.7113Z 2024-11-10T21:29:12.8741Z 1282000\n",
      "Partition 2024-11-10T21:29:12.8751Z 2024-11-10T23:29:21.037Z 1236000\n",
      "Total 5000000\n"
     ]
    }
   ],
   "source": [
    "partition_splits = await get_partition_bounds_backup_results(source_client, timestamp_field_name, desired_partitions=4)\n",
    "total = 0\n",
    "for low, high in await get_partitions(source_client, timestamp_field_name, partition_splits):\n",
    "    partition_size = await get_total_documents_remaining(source_client, timestamp_field_name, low, high)\n",
    "    print(\"Partition\", low, high, partition_size)\n",
    "    total += partition_size\n",
    "print(\"Total\", total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "source_client = SearchClient(source_endpoint, index_name, source_credential)\n",
    "destination_client = SearchClient(destination_endpoint, index_name, destination_credential)\n",
    "\n",
    "backup_file = \"backup-timestamp.txt\"\n",
    "def on_backup_page(last_timestamp: str) -> None:\n",
    "    with open(backup_file, \"w\") as f:\n",
    "        f.write(last_timestamp)\n",
    "\n",
    "restored_timestamp = None\n",
    "if os.path.exists(backup_file):\n",
    "    with open(backup_file, \"r\") as f:\n",
    "        restored_timestamp = f.read()\n",
    "\n",
    "total_documents = await get_total_documents_remaining(\n",
    "    source_client,\n",
    "    timestamp_field_name,\n",
    "    timestamp=restored_timestamp\n",
    ")\n",
    "\n",
    "await backup_index_with_resume(\n",
    "    source_client,\n",
    "    destination_client,\n",
    "    timestamp_field_name,\n",
    "    resume_timestamp=restored_timestamp,\n",
    "    total_documents=total_documents,\n",
    "    on_backup_page=on_backup_page,\n",
    "    backup_tasks=3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
