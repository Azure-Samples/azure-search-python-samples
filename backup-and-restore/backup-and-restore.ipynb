{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity.aio import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "# Variables not used here do not need to be updated in your .env file\n",
    "source_endpoint = os.environ[\"AZURE_SEARCH_SOURCE_SERVICE_ENDPOINT\"]\n",
    "# Using a key is optional. See https://learn.microsoft.com/en-us/azure/search/keyless-connections\n",
    "source_credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_SOURCE_ADMIN_KEY\")) if os.getenv(\"AZURE_SEARCH_SOURCE_ADMIN_KEY\") else DefaultAzureCredential()\n",
    "destination_endpoint = os.environ[\"AZURE_SEARCH_DESTINATION_SERVICE_ENDPOINT\"]\n",
    "destination_credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_DESTINATION_ADMIN_KEY\")) if os.getenv(\"AZURE_SEARCH_DESTINATION_ADMIN_KEY\") else DefaultAzureCredential()\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "timestamp_field_name = os.environ[\"AZURE_SEARCH_TIMESTAMP_FIELD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.aio import SearchIndexClient\n",
    "\n",
    "async def copy_index_definition(source_index_client: SearchIndexClient, destination_index_client: SearchIndexClient, index_name: str):\n",
    "    index = await source_index_client.get_index(index_name)\n",
    "    # Check for any synonym maps\n",
    "    synonym_map_names = []\n",
    "    for field in index.fields:\n",
    "        if field.synonym_map_names:\n",
    "            synonym_map_names.extend(field.synonym_map_names)\n",
    "    \n",
    "    # Copy over synonym maps if they exist\n",
    "    for synonym_map_name in synonym_map_names:\n",
    "        synonym_map = await source_index_client.get_synonym_map(synonym_map_name)\n",
    "        await destination_index_client.create_or_update_synonym_map(synonym_map)\n",
    "    \n",
    "    # Copy over the index\n",
    "    await destination_index_client.create_or_update_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_index_client = SearchIndexClient(endpoint=source_endpoint, credential=source_credential)\n",
    "destination_index_client = SearchIndexClient(endpoint=destination_endpoint, credential=destination_credential)\n",
    "\n",
    "await copy_index_definition(source_index_client, destination_index_client, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.aio import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SearchFieldDataType\n",
    "from typing import List\n",
    "\n",
    "async def validate_resume_backup_and_restore(index_client: SearchIndexClient, index_name: str, timestamp_field_name: str) -> bool:\n",
    "    index = await index_client.get_index(index_name)\n",
    "\n",
    "    found_field = False\n",
    "    for field in index.fields:\n",
    "        if field.name == timestamp_field_name:\n",
    "            found_field = True\n",
    "            if field.type != SearchFieldDataType.DateTimeOffset:\n",
    "                # Field must be a timestamp\n",
    "                return False\n",
    "            if not field.filterable:\n",
    "                # Field must be filterable\n",
    "                return False\n",
    "            if not field.sortable:\n",
    "                # Field must be sortable\n",
    "                return False\n",
    "            break\n",
    "    \n",
    "    # Field must exist on the index\n",
    "    return found_field\n",
    "\n",
    "async def validate_fields_backup_and_restore(index_client: SearchIndexClient, index_name: str) -> List[str]:\n",
    "    missing_fields = []\n",
    "    index = await index_client.get_index(index_name)\n",
    "    for field in index.fields:\n",
    "        message = \"\"\n",
    "        if not field.stored:\n",
    "            message += f\"Field {field.name} cannot be backed up because it's not marked as stored\\n\"\n",
    "        elif field.hidden: \n",
    "            message += f\"Field {field.name} cannot be backed up because it's not marked as retrievable\\n\"\n",
    "        \n",
    "        if message:\n",
    "            missing_fields.append(message)\n",
    "    \n",
    "    return missing_fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_resume_backup_and_restore = await validate_resume_backup_and_restore(source_index_client, index_name, timestamp_field_name)\n",
    "if can_resume_backup_and_restore:\n",
    "    print(\"Index has a valid timestamp field and can use resumable backup and restore\")\n",
    "else:\n",
    "    print(\"Index does not have a valid timestamp field and cannot use resumable backup and restore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.aio import SearchClient\n",
    "from typing import Optional, AsyncGenerator, List, Callable\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "from uuid import uuid4\n",
    "import asyncio\n",
    "\n",
    "async def get_most_recent_timestamp(client: SearchClient, timestamp_field_name: str) -> Optional[str]:\n",
    "    result = await client.search(\n",
    "        search_text=\"*\",\n",
    "        order_by=f\"{timestamp_field_name} desc\",\n",
    "        top=1,\n",
    "        select=[timestamp_field_name]\n",
    "    )\n",
    "    if len(result) == 0:\n",
    "        return None\n",
    "    return result[0][timestamp_field_name]\n",
    "\n",
    "async def resume_backup_results(client: SearchClient, timestamp_field_name: str, timestamp: Optional[str], select=None) -> AsyncGenerator[List[dict], None]:\n",
    "    session_id = str(uuid4())\n",
    "    max_results_size = 100000\n",
    "    get_next_results = True\n",
    "    while get_next_results:\n",
    "        total_results_size = 0\n",
    "        results = await client.search(\n",
    "            search_text=\"*\",\n",
    "            order_by=f\"{timestamp_field_name} asc\",\n",
    "            top=max_results_size,\n",
    "            filter=f\"{timestamp_field_name} ge {timestamp}\" if timestamp else None,\n",
    "            session_id=session_id,\n",
    "            select=select\n",
    "        )\n",
    "        results_by_page = results.by_page()\n",
    "\n",
    "        async for page in results_by_page:\n",
    "            next_page = [item async for item in page]\n",
    "            total_results_size += len(next_page)\n",
    "            yield next_page\n",
    "            timestamp = next_page[-1][timestamp_field_name]\n",
    "        \n",
    "        get_next_results = total_results_size == max_results_size\n",
    "\n",
    "async def get_key_field(index_client: SearchIndexClient, index_name: str) -> str:\n",
    "    index = await index_client.get_index(index_name)\n",
    "    for field in index.fields:\n",
    "        if field.key:\n",
    "            return field.name\n",
    "    \n",
    "    raise Exception(\"No key field in the index\")\n",
    "\n",
    "async def get_total_documents_remaining(source_client: SearchClient, timestamp_field_name: str, timestamp: Optional[str]) -> int:\n",
    "    results = await source_client.search(\n",
    "        search_text=\"*\",\n",
    "        include_total_count=True,\n",
    "        filter=f\"{timestamp_field_name} ge {timestamp}\" if timestamp else None,\n",
    "        top=0\n",
    "    )\n",
    "    return await results.get_count()\n",
    "\n",
    "## FIXME\n",
    "def find_missing_backup_keys(source_client: SearchClient, destination_client: SearchClient, timestamp_field_name: str, key_field_name: str) -> List[str]:\n",
    "    source_results = resume_backup_results(source_client, timestamp_field_name, select=[key_field_name])\n",
    "    destination_results = resume_backup_results(destination_client, timestamp_field_name, select=[key_field_name])\n",
    "    missing_destination_keys = []\n",
    "    for source_page, destination_page in zip(source_results, destination_results):\n",
    "        missing_destination_page_keys = set(source_page) - set(destination_page)\n",
    "        missing_destination_keys.extend(missing_destination_page_keys)\n",
    "    \n",
    "    return missing_destination_keys\n",
    "\n",
    "async def backup_index_with_resume(source_client: SearchClient, destination_client: SearchClient, timestamp_field_name: str, total_documents: int, resume_timestamp: Optional[str], backup_tasks:int = 2, on_backup_page: Optional[Callable[[str], None]] = None) -> None:\n",
    "    progress_bar = tqdm(total=total_documents, desc=\"Backing up documents...\", unit=\"docs\", unit_scale=False)\n",
    "    pages_label = widgets.Label(value=\"Queued Result Pages: 0\")\n",
    "    display(pages_label)\n",
    "    \n",
    "    async def get_results(output_queue: asyncio.Queue):\n",
    "        results = resume_backup_results(source_client, timestamp_field_name, timestamp=resume_timestamp)\n",
    "        async for result_page in results:\n",
    "            pages_label.value=f\"Queued Result Pages: {output_queue.qsize()}\"\n",
    "            await output_queue.put(result_page)\n",
    "        await output_queue.put(None)\n",
    "    \n",
    "    async def backup_results(results_queue: asyncio.Queue, timestamp_queue: asyncio.Queue):\n",
    "        processed_count = 0\n",
    "        while True:\n",
    "            result_page = await results_queue.get()\n",
    "            if result_page is None:\n",
    "                break\n",
    "            saved_timestamp = result_page[-1][timestamp_field_name]\n",
    "            await destination_client.upload_documents(result_page)\n",
    "            await timestamp_queue.put(saved_timestamp)\n",
    "            processed_count += len(result_page)\n",
    "            progress_bar.update(len(result_page))\n",
    "    \n",
    "    async def checkpoint_results(timestamp_queue: asyncio.Queue):\n",
    "        latest_timestamp = None\n",
    "        while True:\n",
    "            next_timestamp = await timestamp_queue.get()\n",
    "            if next_timestamp is None:  # Stop signal received\n",
    "                break\n",
    "            if latest_timestamp is None or next_timestamp > latest_timestamp:\n",
    "                latest_timestamp = next_timestamp\n",
    "                on_backup_page(latest_timestamp)\n",
    "\n",
    "    results_queue = asyncio.Queue()\n",
    "    timestamp_queue = asyncio.Queue()\n",
    "\n",
    "    # Run producer and consumer concurrently\n",
    "    producer_task = asyncio.create_task(get_results(results_queue))\n",
    "    \n",
    "    consumer_tasks = [asyncio.create_task(backup_results(results_queue, timestamp_queue)) for i in range(backup_tasks)]\n",
    "\n",
    "    checkpoint_task = asyncio.create_task(checkpoint_results(timestamp_queue))\n",
    "\n",
    "    # Wait for the producer to complete\n",
    "    await producer_task\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    await producer_task\n",
    "    await asyncio.gather(*consumer_tasks)\n",
    "    await checkpoint_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "source_client = SearchClient(source_endpoint, index_name, source_credential)\n",
    "destination_client = SearchClient(destination_endpoint, index_name, destination_credential)\n",
    "\n",
    "backup_file = \"backup-timestamp.txt\"\n",
    "def on_backup_page(last_timestamp: str) -> None:\n",
    "    with open(backup_file, \"w\") as f:\n",
    "        f.write(last_timestamp)\n",
    "\n",
    "restored_timestamp = None\n",
    "if os.path.exists(backup_file):\n",
    "    with open(backup_file, \"r\") as f:\n",
    "        restored_timestamp = f.read()\n",
    "\n",
    "total_documents = await get_total_documents_remaining(\n",
    "    source_client,\n",
    "    timestamp_field_name,\n",
    "    timestamp=restored_timestamp\n",
    ")\n",
    "\n",
    "await backup_index_with_resume(\n",
    "    source_client,\n",
    "    destination_client,\n",
    "    timestamp_field_name,\n",
    "    resume_timestamp=restored_timestamp,\n",
    "    total_documents=total_documents,\n",
    "    on_backup_page=on_backup_page,\n",
    "    backup_tasks=3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
