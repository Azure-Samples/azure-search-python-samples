{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichment sample in Python for Azure AI Search\n",
    "\n",
    "In this Jupyter Notebook, create and run enrichment steps to unlock searchable content in Azure blobs. It performs operations over mixed content in Azure Storage, such as images and application files, using a skillset that analyzes and extracts text information that becomes searchable in Azure AI Search. The full documentation for this sample can be found at [Tutorial: Use Python and AI to generate searchable content from Azure blobs](https://docs.microsoft.com/azure/search/cognitive-search-tutorial-blob-python).\n",
    "\n",
    "This sample creates the following objects on your search service:\n",
    "\n",
    "+ search index\n",
    "+ data source\n",
    "+ skillset\n",
    "+ indexer\n",
    "\n",
    "In the last step, you'll run queries against the search index to explore the text output that was generated for each blob.\n",
    "\n",
    "This notebook calls the [Search REST APIs](https://docs.microsoft.com/rest/api/searchservice/), but you can also use the Azure.Search.Documents client library in the Azure SDK for Python to perform the same steps. See this [Python quickstart](https://docs.microsoft.com/azure/search/search-get-started-python) for details.\n",
    "\n",
    "To run this sample, satisfy all [prerequisites](https://docs.microsoft.com/azure/search/cognitive-search-tutorial-blob-python#prerequisites) stated in the tutorial and upload the sample data to a blob container in Azure Storage account. In this notebook, replace the placeholders for the search service endpoint, the admin API key, Azure Storage connection string, and blob container. Once you've provided all four values, you can run all cells, but the query won't return results until the indexer is finished and the search index is loaded. \n",
    "\n",
    "We recommend running each step and making sure it completes before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the names for the data source, skillset, index and indexer\n",
    "datasource_name = \"cogsrch-py-datasource\"\n",
    "skillset_name = \"cogsrch-py-skillset\"\n",
    "index_name = \"cogsrch-py-index\"\n",
    "indexer_name = \"cogsrch-py-indexer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the endpoint\n",
    "endpoint = 'https://<YOUR-SEARCH-SERVICE-NAME>.search.windows.net/'\n",
    "headers = {'Content-Type': 'application/json',\n",
    "           'api-key': '<YOUR-ADMIN-API-KEY>'}\n",
    "params = {\n",
    "    'api-version': '2020-06-30'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data source\n",
    "# This data source points to your Azure Storage account.\n",
    "# You should already have a blob container that contains the sample data\n",
    "datasourceConnectionString = \"<YOUR-BLOB-RESOURCE-CONNECTION-STRING>\"\n",
    "datasource_payload = {\n",
    "    \"name\": datasource_name,\n",
    "    \"description\": \"Demo files to demonstrate search capabilities.\",\n",
    "    \"type\": \"azureblob\",\n",
    "    \"credentials\": {\n",
    "        \"connectionString\": datasourceConnectionString\n",
    "    },\n",
    "    \"container\": {\n",
    "        \"name\": \"<YOUR-BLOB-CONTAINER-NAME>\"\n",
    "    }\n",
    "}\n",
    "r = requests.put(endpoint + \"/datasources/\" + datasource_name,\n",
    "                 data=json.dumps(datasource_payload), headers=headers, params=params)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a skillset\n",
    "skillset_payload = {\n",
    "    \"name\": skillset_name,\n",
    "    \"description\":\n",
    "    \"Extract entities, detect language and extract key-phrases\",\n",
    "    \"skills\":\n",
    "    [\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.V3.EntityRecognitionSkill\",\n",
    "            \"categories\": [\"Organization\"],\n",
    "            \"defaultLanguageCode\": \"en\",\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"name\": \"text\", \n",
    "                    \"source\": \"/document/content\"\n",
    "                }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"organizations\", \n",
    "                    \"targetName\": \"organizations\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.LanguageDetectionSkill\",\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"name\": \"text\", \n",
    "                    \"source\": \"/document/content\"\n",
    "                }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"languageCode\",\n",
    "                    \"targetName\": \"languageCode\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.SplitSkill\",\n",
    "            \"textSplitMode\": \"pages\",\n",
    "            \"maximumPageLength\": 4000,\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"name\": \"text\",\n",
    "                    \"source\": \"/document/content\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"languageCode\",\n",
    "                    \"source\": \"/document/languageCode\"\n",
    "                }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"textItems\",\n",
    "                    \"targetName\": \"pages\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.KeyPhraseExtractionSkill\",\n",
    "            \"context\": \"/document/pages/*\",\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"name\": \"text\", \n",
    "                    \"source\": \"/document/pages/*\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"languageCode\", \n",
    "                    \"source\": \"/document/languageCode\"\n",
    "                }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"keyPhrases\",\n",
    "                    \"targetName\": \"keyPhrases\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "r = requests.put(endpoint + \"/skillsets/\" + skillset_name,\n",
    "                 data=json.dumps(skillset_payload), headers=headers, params=params)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index\n",
    "# Queries operate over the searchable fields and filterable fields in the index\n",
    "index_payload = {\n",
    "    \"name\": index_name,\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"id\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"key\": \"true\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"facetable\": \"false\",\n",
    "            \"sortable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"content\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"sortable\": \"false\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"languageCode\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"keyPhrases\",\n",
    "            \"type\": \"Collection(Edm.String)\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"organizations\",\n",
    "            \"type\": \"Collection(Edm.String)\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"sortable\": \"false\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"facetable\": \"false\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "r = requests.put(endpoint + \"/indexes/\" + index_name,\n",
    "                 data=json.dumps(index_payload), headers=headers, params=params)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexer\n",
    "indexer_payload = {\n",
    "    \"name\": indexer_name,\n",
    "    \"dataSourceName\": datasource_name,\n",
    "    \"targetIndexName\": index_name,\n",
    "    \"skillsetName\": skillset_name,\n",
    "    \"fieldMappings\": [\n",
    "        {\n",
    "            \"sourceFieldName\": \"metadata_storage_path\",\n",
    "            \"targetFieldName\": \"id\",\n",
    "            \"mappingFunction\":\n",
    "            {\"name\": \"base64Encode\"}\n",
    "        },\n",
    "        {\n",
    "            \"sourceFieldName\": \"content\",\n",
    "            \"targetFieldName\": \"content\"\n",
    "        }\n",
    "    ],\n",
    "    \"outputFieldMappings\":\n",
    "    [\n",
    "        {\n",
    "            \"sourceFieldName\": \"/document/organizations\",\n",
    "            \"targetFieldName\": \"organizations\"\n",
    "        },\n",
    "        {\n",
    "            \"sourceFieldName\": \"/document/pages/*/keyPhrases/*\",\n",
    "            \"targetFieldName\": \"keyPhrases\"\n",
    "        },\n",
    "        {\n",
    "            \"sourceFieldName\": \"/document/languageCode\",\n",
    "            \"targetFieldName\": \"languageCode\"\n",
    "        }\n",
    "    ],\n",
    "    \"parameters\":\n",
    "    {\n",
    "        \"maxFailedItems\": -1,\n",
    "        \"maxFailedItemsPerBatch\": -1,\n",
    "        \"configuration\":\n",
    "        {\n",
    "            \"dataToExtract\": \"contentAndMetadata\",\n",
    "            \"imageAction\": \"generateNormalizedImages\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "r = requests.put(endpoint + \"/indexers/\" + indexer_name,\n",
    "                 data=json.dumps(indexer_payload), headers=headers, params=params)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, get indexer status to confirm that it's running\n",
    "r = requests.get(endpoint + \"/indexers/\" + indexer_name +\n",
    "                 \"/status\", headers=headers, params=params)\n",
    "pprint(json.dumps(r.json(), indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the service for the index definition\n",
    "# Query responses can be verbose. If you get \"Output exceeds the size limit. Open the full output data in a text editor\", open the output in an editor.\n",
    "r = requests.get(endpoint + \"/indexes/\" + index_name,\n",
    "                 headers=headers, params=params)\n",
    "pprint(json.dumps(r.json(), indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the index to return the contents of \"organizations\", created through Entity Recognition during enrichment\n",
    "# For keyword search, replace the asterisk with comma-separated query terms: search=microsoft,azure\n",
    "r = requests.get(endpoint + \"/indexes/\" + index_name +\n",
    "                 \"/docs?&search=*&$select=organizations\", headers=headers, params=params)\n",
    "pprint(json.dumps(r.json(), indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step concludes the walkthrough, showing you how to invoke skills that extract searchable content and information from image and application files in Azure Storage. The processes run on Azure AI Search, and the resulting index can be used in client apps to provide a search experience. For more information, visit the [official docs](https://docs.microsoft.com/azure/search/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ff083f0c83558f9261023d47a77b9b3eb892c62cdbe066d046abcad1a5edb5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
